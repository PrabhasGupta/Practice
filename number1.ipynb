{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10960485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\prabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\prabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64575b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "405f9089",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (842801469.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python.exe -m pip install --upgrade pip\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d1ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e4ee66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.2-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "     ---------------------------------------- 10.3/10.3 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\prabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "     -------------------------------------- 499.4/499.4 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.21.0\n",
      "  Downloading numpy-1.24.1-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "     ---------------------------------------- 14.8/14.8 MB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.24.1 pandas-1.5.2 pytz-2022.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0f967c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prabhas</td>\n",
       "      <td>22</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vignesh</td>\n",
       "      <td>23</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Khaleel</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harsha</td>\n",
       "      <td>23</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JayPrakash</td>\n",
       "      <td>22</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vamshi</td>\n",
       "      <td>25</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shiva</td>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Manohar</td>\n",
       "      <td>21</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shashank</td>\n",
       "      <td>23</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vijay</td>\n",
       "      <td>21</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naveen</td>\n",
       "      <td>26</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Goutham</td>\n",
       "      <td>28</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Harish</td>\n",
       "      <td>27</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sravan</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Teja</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Vikram</td>\n",
       "      <td>20</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kiran</td>\n",
       "      <td>24</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Abhilash</td>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rahul</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Varshith</td>\n",
       "      <td>28</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kaushik</td>\n",
       "      <td>19</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Anurag</td>\n",
       "      <td>18</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Vishwa</td>\n",
       "      <td>17</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ravindranadh</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Madhu</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Akhil</td>\n",
       "      <td>27</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Kayode</td>\n",
       "      <td>23</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Rohit</td>\n",
       "      <td>21</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ram</td>\n",
       "      <td>31</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Satwik</td>\n",
       "      <td>24</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Dinesh</td>\n",
       "      <td>25</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tarun</td>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Nikhil</td>\n",
       "      <td>21</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Rohan</td>\n",
       "      <td>27</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Pavan</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sampreeth</td>\n",
       "      <td>26</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Varun</td>\n",
       "      <td>20</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Rahul</td>\n",
       "      <td>20</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Naveen</td>\n",
       "      <td>17</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Vijay</td>\n",
       "      <td>29</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Age  Weight\n",
       "0        Prabhas   22      90\n",
       "1        Vignesh   23      75\n",
       "2        Khaleel   24      74\n",
       "3         Harsha   23      78\n",
       "4     JayPrakash   22      65\n",
       "5         Vamshi   25      70\n",
       "6          Shiva   25      66\n",
       "7        Manohar   21      71\n",
       "8       shashank   23      70\n",
       "9          Vijay   21      70\n",
       "10        Naveen   26      60\n",
       "11       Goutham   28      88\n",
       "12        Harish   27      87\n",
       "13        Sravan   22      55\n",
       "14          Teja   20      55\n",
       "15        Vikram   20      57\n",
       "16         Kiran   24      60\n",
       "17      Abhilash   22      60\n",
       "18         Rahul   30      80\n",
       "19      Varshith   28      76\n",
       "20       Kaushik   19      86\n",
       "21        Anurag   18      87\n",
       "22        Vishwa   17      66\n",
       "23  Ravindranadh   20      64\n",
       "24         Madhu   26      85\n",
       "25         Akhil   27      85\n",
       "26        Kayode   23      65\n",
       "27         Rohit   21      80\n",
       "28           Ram   31      89\n",
       "29        Satwik   24      87\n",
       "30        Dinesh   25      80\n",
       "31         Tarun   21      50\n",
       "32        Nikhil   21      89\n",
       "33         Rohan   27      80\n",
       "34         Pavan   19      55\n",
       "35     Sampreeth   26      89\n",
       "36         Varun   20      80\n",
       "37         Rahul   20      82\n",
       "38        Naveen   17      57\n",
       "39         Vijay   29      85"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"NameAge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d019da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcbe444",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b147e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.224:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x214aa7d9c70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e301208f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o45.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (10.0.0.224 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9388\\193180192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Firstname\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Lastname\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Country\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"State\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    960\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (10.0.0.224 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Rishi\",\"Rageer\",\"USA\",\"FL\"),(\"Yashwanth\",\"Kundarapu\",\"USA\",\"FL\"),(\"Eshwar\",\"Ambati\",\"USA\",\"NJ\"),(\"Karthik\",\"Bantupalli\",\"USA\",\"WI\")]\n",
    "columns=[\"Firstname\",\"Lastname\",\"Country\",\"State\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "print(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17485943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('NameAge.csv')\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "438cc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option(\"header\",\"true\").csv(\"NameAge.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2e7a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68b87432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70524e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|      Name|Age|Weight|\n",
      "+----------+---+------+\n",
      "|   Prabhas| 22|    90|\n",
      "|   Vignesh| 23|    75|\n",
      "|   Khaleel| 24|    74|\n",
      "|    Harsha| 23|    78|\n",
      "|JayPrakash| 22|    65|\n",
      "|    Vamshi| 25|    70|\n",
      "|     Shiva| 25|    66|\n",
      "|   Manohar| 21|    71|\n",
      "|  shashank| 23|    70|\n",
      "|     Vijay| 21|    70|\n",
      "|    Naveen| 26|    60|\n",
      "|   Goutham| 28|    88|\n",
      "|    Harish| 27|    87|\n",
      "|    Sravan| 22|    55|\n",
      "|      Teja| 20|    55|\n",
      "|    Vikram| 20|    57|\n",
      "|     Kiran| 24|    60|\n",
      "|  Abhilash| 22|    60|\n",
      "|     Rahul| 30|    80|\n",
      "|  Varshith| 28|    76|\n",
      "+----------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##actual type to read .csv files\n",
    "df_pyspark=spark.read.csv(\"NameAge.csv\",header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19f92f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76732945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      Name|Weight|\n",
      "+----------+------+\n",
      "|   Prabhas|    90|\n",
      "|   Vignesh|    75|\n",
      "|   Khaleel|    74|\n",
      "|    Harsha|    78|\n",
      "|JayPrakash|    65|\n",
      "|    Vamshi|    70|\n",
      "|     Shiva|    66|\n",
      "|   Manohar|    71|\n",
      "|  shashank|    70|\n",
      "|     Vijay|    70|\n",
      "|    Naveen|    60|\n",
      "|   Goutham|    88|\n",
      "|    Harish|    87|\n",
      "|    Sravan|    55|\n",
      "|      Teja|    55|\n",
      "|    Vikram|    57|\n",
      "|     Kiran|    60|\n",
      "|  Abhilash|    60|\n",
      "|     Rahul|    80|\n",
      "|  Varshith|    76|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select([\"Name\",\"Weight\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8f3455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Weight', 'int')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking datatypes\n",
    "\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dc6d98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+------------------+\n",
      "|summary|    Name|               Age|            Weight|\n",
      "+-------+--------+------------------+------------------+\n",
      "|  count|      40|                40|                40|\n",
      "|   mean|    null|            23.175|              73.7|\n",
      "| stddev|    null|3.5292731873247267|12.062231795875748|\n",
      "|    min|Abhilash|                17|                50|\n",
      "|    max|shashank|                31|                90|\n",
      "+-------+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3e88813",
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding the columns\n",
    "df_pyspark=df_pyspark.withColumn(\"After 2 years\",df_pyspark[\"Age\"]+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f07ce6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------------+\n",
      "|      Name|Age|Weight|After 2 years|\n",
      "+----------+---+------+-------------+\n",
      "|   Prabhas| 22|    90|           24|\n",
      "|   Vignesh| 23|    75|           25|\n",
      "|   Khaleel| 24|    74|           26|\n",
      "|    Harsha| 23|    78|           25|\n",
      "|JayPrakash| 22|    65|           24|\n",
      "|    Vamshi| 25|    70|           27|\n",
      "|     Shiva| 25|    66|           27|\n",
      "|   Manohar| 21|    71|           23|\n",
      "|  shashank| 23|    70|           25|\n",
      "|     Vijay| 21|    70|           23|\n",
      "|    Naveen| 26|    60|           28|\n",
      "|   Goutham| 28|    88|           30|\n",
      "|    Harish| 27|    87|           29|\n",
      "|    Sravan| 22|    55|           24|\n",
      "|      Teja| 20|    55|           22|\n",
      "|    Vikram| 20|    57|           22|\n",
      "|     Kiran| 24|    60|           26|\n",
      "|  Abhilash| 22|    60|           24|\n",
      "|     Rahul| 30|    80|           32|\n",
      "|  Varshith| 28|    76|           30|\n",
      "+----------+---+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "498e1e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|      Name|Age|Weight|\n",
      "+----------+---+------+\n",
      "|   Prabhas| 22|    90|\n",
      "|   Vignesh| 23|    75|\n",
      "|   Khaleel| 24|    74|\n",
      "|    Harsha| 23|    78|\n",
      "|JayPrakash| 22|    65|\n",
      "|    Vamshi| 25|    70|\n",
      "|     Shiva| 25|    66|\n",
      "|   Manohar| 21|    71|\n",
      "|  shashank| 23|    70|\n",
      "|     Vijay| 21|    70|\n",
      "|    Naveen| 26|    60|\n",
      "|   Goutham| 28|    88|\n",
      "|    Harish| 27|    87|\n",
      "|    Sravan| 22|    55|\n",
      "|      Teja| 20|    55|\n",
      "|    Vikram| 20|    57|\n",
      "|     Kiran| 24|    60|\n",
      "|  Abhilash| 22|    60|\n",
      "|     Rahul| 30|    80|\n",
      "|  Varshith| 28|    76|\n",
      "+----------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## dropping the columns\n",
    "df_pyspark=df_pyspark.drop(\"After 2 years\")\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1119c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|  New Name|Age|Weight|\n",
      "+----------+---+------+\n",
      "|   Prabhas| 22|    90|\n",
      "|   Vignesh| 23|    75|\n",
      "|   Khaleel| 24|    74|\n",
      "|    Harsha| 23|    78|\n",
      "|JayPrakash| 22|    65|\n",
      "|    Vamshi| 25|    70|\n",
      "|     Shiva| 25|    66|\n",
      "|   Manohar| 21|    71|\n",
      "|  shashank| 23|    70|\n",
      "|     Vijay| 21|    70|\n",
      "|    Naveen| 26|    60|\n",
      "|   Goutham| 28|    88|\n",
      "|    Harish| 27|    87|\n",
      "|    Sravan| 22|    55|\n",
      "|      Teja| 20|    55|\n",
      "|    Vikram| 20|    57|\n",
      "|     Kiran| 24|    60|\n",
      "|  Abhilash| 22|    60|\n",
      "|     Rahul| 30|    80|\n",
      "|  Varshith| 28|    76|\n",
      "+----------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Renaming the columns\n",
    "df_pyspark.withColumnRenamed(\"Name\",\"New Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ec25515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+\n",
      "|        Name| Age|Weight|\n",
      "+------------+----+------+\n",
      "|     Prabhas|  22|    90|\n",
      "|     Vignesh|  23|    75|\n",
      "|     Khaleel|null|    74|\n",
      "|      Harsha|  23|    78|\n",
      "|  JayPrakash|  22|    65|\n",
      "|      Vamshi|  25|    70|\n",
      "|    shashank|  23|  null|\n",
      "|       Vijay|  21|    70|\n",
      "|      Sravan|  22|    55|\n",
      "|        Teja|null|    55|\n",
      "|    Abhilash|  22|    60|\n",
      "|Ravindranadh|  20|  null|\n",
      "|      Kayode|  23|    65|\n",
      "|       Rohit|  21|  null|\n",
      "|      Nikhil|null|    89|\n",
      "|       Rohan|  27|    80|\n",
      "|      Naveen|  17|    57|\n",
      "|       Vijay|null|    85|\n",
      "|        null|  56|  null|\n",
      "|        null|null|   100|\n",
      "+------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv(\"test1.csv\",header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ac4c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|      Name|Age|Weight|\n",
      "+----------+---+------+\n",
      "|   Prabhas| 22|    90|\n",
      "|   Vignesh| 23|    75|\n",
      "|    Harsha| 23|    78|\n",
      "|JayPrakash| 22|    65|\n",
      "|    Vamshi| 25|    70|\n",
      "|     Vijay| 21|    70|\n",
      "|    Sravan| 22|    55|\n",
      "|  Abhilash| 22|    60|\n",
      "|    Kayode| 23|    65|\n",
      "|     Rohan| 27|    80|\n",
      "|    Naveen| 17|    57|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8a5d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+\n",
      "|        Name| Age|Weight|\n",
      "+------------+----+------+\n",
      "|     Prabhas|  22|    90|\n",
      "|     Vignesh|  23|    75|\n",
      "|     Khaleel|null|    74|\n",
      "|      Harsha|  23|    78|\n",
      "|  JayPrakash|  22|    65|\n",
      "|      Vamshi|  25|    70|\n",
      "|    shashank|  23|  null|\n",
      "|       Vijay|  21|    70|\n",
      "|      Sravan|  22|    55|\n",
      "|        Teja|null|    55|\n",
      "|    Abhilash|  22|    60|\n",
      "|Ravindranadh|  20|  null|\n",
      "|      Kayode|  23|    65|\n",
      "|       Rohit|  21|  null|\n",
      "|      Nikhil|null|    89|\n",
      "|       Rohan|  27|    80|\n",
      "|      Naveen|  17|    57|\n",
      "|       Vijay|null|    85|\n",
      "|        null|  56|  null|\n",
      "|        null|null|   100|\n",
      "+------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop(how=\"all\")\n",
    "df_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c4f8dc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|      Name|Age|Weight|\n",
      "+----------+---+------+\n",
      "|   Prabhas| 22|    90|\n",
      "|   Vignesh| 23|    75|\n",
      "|    Harsha| 23|    78|\n",
      "|JayPrakash| 22|    65|\n",
      "|    Vamshi| 25|    70|\n",
      "|     Vijay| 21|    70|\n",
      "|    Sravan| 22|    55|\n",
      "|  Abhilash| 22|    60|\n",
      "|    Kayode| 23|    65|\n",
      "|     Rohan| 27|    80|\n",
      "|    Naveen| 17|    57|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop(how=\"any\")-- same as default drop()\n",
    "df_pyspark.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "931968ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+\n",
      "|        Name| Age|Weight|\n",
      "+------------+----+------+\n",
      "|     Prabhas|  22|    90|\n",
      "|     Vignesh|  23|    75|\n",
      "|     Khaleel|null|    74|\n",
      "|      Harsha|  23|    78|\n",
      "|  JayPrakash|  22|    65|\n",
      "|      Vamshi|  25|    70|\n",
      "|    shashank|  23|  null|\n",
      "|       Vijay|  21|    70|\n",
      "|      Sravan|  22|    55|\n",
      "|        Teja|null|    55|\n",
      "|    Abhilash|  22|    60|\n",
      "|Ravindranadh|  20|  null|\n",
      "|      Kayode|  23|    65|\n",
      "|       Rohit|  21|  null|\n",
      "|      Nikhil|null|    89|\n",
      "|       Rohan|  27|    80|\n",
      "|      Naveen|  17|    57|\n",
      "|       Vijay|null|    85|\n",
      "+------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshold\n",
    "df_pyspark.na.drop(how=\"any\",thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "832bda36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+\n",
      "|        Name|Age|Weight|\n",
      "+------------+---+------+\n",
      "|     Prabhas| 22|    90|\n",
      "|     Vignesh| 23|    75|\n",
      "|      Harsha| 23|    78|\n",
      "|  JayPrakash| 22|    65|\n",
      "|      Vamshi| 25|    70|\n",
      "|    shashank| 23|  null|\n",
      "|       Vijay| 21|    70|\n",
      "|      Sravan| 22|    55|\n",
      "|    Abhilash| 22|    60|\n",
      "|Ravindranadh| 20|  null|\n",
      "|      Kayode| 23|    65|\n",
      "|       Rohit| 21|  null|\n",
      "|       Rohan| 27|    80|\n",
      "|      Naveen| 17|    57|\n",
      "|        null| 56|  null|\n",
      "+------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## subset\n",
    "df_pyspark.na.drop(how=\"any\",subset=['Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "008a8c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+------+\n",
      "|          Name| Age|Weight|\n",
      "+--------------+----+------+\n",
      "|       Prabhas|  22|    90|\n",
      "|       Vignesh|  23|    75|\n",
      "|       Khaleel|null|    74|\n",
      "|        Harsha|  23|    78|\n",
      "|    JayPrakash|  22|    65|\n",
      "|        Vamshi|  25|    70|\n",
      "|      shashank|  23|  null|\n",
      "|         Vijay|  21|    70|\n",
      "|        Sravan|  22|    55|\n",
      "|          Teja|null|    55|\n",
      "|      Abhilash|  22|    60|\n",
      "|  Ravindranadh|  20|  null|\n",
      "|        Kayode|  23|    65|\n",
      "|         Rohit|  21|  null|\n",
      "|        Nikhil|null|    89|\n",
      "|         Rohan|  27|    80|\n",
      "|        Naveen|  17|    57|\n",
      "|         Vijay|null|    85|\n",
      "|Missing values|  56|  null|\n",
      "|Missing values|null|   100|\n",
      "+--------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filling the missing values with \"Missing values\"\n",
    "df_pyspark.na.fill(\"Missing values\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a382882",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imputer function to impute values, we can use mode and median also in place of setStrategy(\"mean\")\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(\n",
    "                inputCols=[\"Age\",\"Weight\"],\n",
    "                outputCols=[\"{}_imputed\".format(c) for c in [\"Age\",\"Weight\"]]).setStrategy(\"mean\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d6d33baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+-----------+--------------+\n",
      "|        Name| Age|Weight|Age_imputed|Weight_imputed|\n",
      "+------------+----+------+-----------+--------------+\n",
      "|     Prabhas|  22|    90|         22|            90|\n",
      "|     Vignesh|  23|    75|         23|            75|\n",
      "|     Khaleel|null|    74|         24|            74|\n",
      "|      Harsha|  23|    78|         23|            78|\n",
      "|  JayPrakash|  22|    65|         22|            65|\n",
      "|      Vamshi|  25|    70|         25|            70|\n",
      "|    shashank|  23|  null|         23|            73|\n",
      "|       Vijay|  21|    70|         21|            70|\n",
      "|      Sravan|  22|    55|         22|            55|\n",
      "|        Teja|null|    55|         24|            55|\n",
      "|    Abhilash|  22|    60|         22|            60|\n",
      "|Ravindranadh|  20|  null|         20|            73|\n",
      "|      Kayode|  23|    65|         23|            65|\n",
      "|       Rohit|  21|  null|         21|            73|\n",
      "|      Nikhil|null|    89|         24|            89|\n",
      "|       Rohan|  27|    80|         27|            80|\n",
      "|      Naveen|  17|    57|         17|            57|\n",
      "|       Vijay|null|    85|         24|            85|\n",
      "|        null|  56|  null|         56|            73|\n",
      "|        null|null|   100|         24|           100|\n",
      "+------------+----+------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## using imputer function, fit(), transform()\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f198860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"practise\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f60cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv(\"NameAge.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a1e53f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+\n",
      "|      Name|Age|Weight|\n",
      "+----------+---+------+\n",
      "|   Prabhas| 22|    90|\n",
      "|   Vignesh| 23|    75|\n",
      "|   Khaleel| 24|    74|\n",
      "|    Harsha| 23|    78|\n",
      "|JayPrakash| 22|    65|\n",
      "|    Vamshi| 25|    70|\n",
      "|     Shiva| 25|    66|\n",
      "|   Manohar| 21|    71|\n",
      "|  shashank| 23|    70|\n",
      "|     Vijay| 21|    70|\n",
      "|    Naveen| 26|    60|\n",
      "|   Goutham| 28|    88|\n",
      "|    Harish| 27|    87|\n",
      "|    Sravan| 22|    55|\n",
      "|      Teja| 20|    55|\n",
      "|    Vikram| 20|    57|\n",
      "|     Kiran| 24|    60|\n",
      "|  Abhilash| 22|    60|\n",
      "|     Rahul| 30|    80|\n",
      "|  Varshith| 28|    76|\n",
      "+----------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e27cb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     Name|Age|Weight|\n",
      "+---------+---+------+\n",
      "|   Vamshi| 25|    70|\n",
      "|    Shiva| 25|    66|\n",
      "|   Naveen| 26|    60|\n",
      "|  Goutham| 28|    88|\n",
      "|   Harish| 27|    87|\n",
      "|    Rahul| 30|    80|\n",
      "| Varshith| 28|    76|\n",
      "|    Madhu| 26|    85|\n",
      "|    Akhil| 27|    85|\n",
      "|      Ram| 31|    89|\n",
      "|   Dinesh| 25|    80|\n",
      "|    Rohan| 27|    80|\n",
      "|Sampreeth| 26|    89|\n",
      "|    Vijay| 29|    85|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### filter operations\n",
    "df_pyspark.filter(\"age>=25\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870298a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|   Naveen|\n",
      "|  Goutham|\n",
      "|   Harish|\n",
      "|    Rahul|\n",
      "| Varshith|\n",
      "|    Madhu|\n",
      "|    Akhil|\n",
      "|      Ram|\n",
      "|    Rohan|\n",
      "|Sampreeth|\n",
      "|    Vijay|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"age>25\").select([\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b8e94a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     Name|Age|Weight|\n",
      "+---------+---+------+\n",
      "|   Naveen| 26|    60|\n",
      "|  Goutham| 28|    88|\n",
      "|   Harish| 27|    87|\n",
      "| Varshith| 28|    76|\n",
      "|    Madhu| 26|    85|\n",
      "|    Akhil| 27|    85|\n",
      "|    Rohan| 27|    80|\n",
      "|Sampreeth| 26|    89|\n",
      "|    Vijay| 29|    85|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark[\"age\"]>25) & (df_pyspark[\"age\"]<30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33b050dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1e02a5dc350>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy- we grouped to find the weights of person\n",
    "df_pyspark.groupBy(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c412199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|        Name|sum(weight)|\n",
      "+------------+-----------+\n",
      "|       Shiva|         66|\n",
      "|      Vishwa|         66|\n",
      "|      Dinesh|         80|\n",
      "|    shashank|         70|\n",
      "|     Prabhas|         90|\n",
      "|   Sampreeth|         89|\n",
      "|      Harish|         87|\n",
      "|     Manohar|         71|\n",
      "|     Khaleel|         74|\n",
      "|      Sravan|         55|\n",
      "|     Vignesh|         75|\n",
      "|         Ram|         89|\n",
      "|       Tarun|         50|\n",
      "|Ravindranadh|         64|\n",
      "|  JayPrakash|         65|\n",
      "|       Pavan|         55|\n",
      "|     Goutham|         88|\n",
      "|       Madhu|         85|\n",
      "|      Kayode|         65|\n",
      "|       Vijay|        155|\n",
      "+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Name\").sum(\"weight\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "876bc0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|Age|sum(weight)|\n",
      "+---+-----------+\n",
      "| 31|         89|\n",
      "| 28|        164|\n",
      "| 26|        234|\n",
      "| 27|        252|\n",
      "| 22|        270|\n",
      "| 20|        338|\n",
      "| 19|        141|\n",
      "| 17|        123|\n",
      "| 23|        288|\n",
      "| 25|        216|\n",
      "| 24|        221|\n",
      "| 29|         85|\n",
      "| 21|        360|\n",
      "| 30|         80|\n",
      "| 18|         87|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Age\").sum(\"weight\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d2b5e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|Age|      avg(weight)|\n",
      "+---+-----------------+\n",
      "| 31|             89.0|\n",
      "| 28|             82.0|\n",
      "| 26|             78.0|\n",
      "| 27|             84.0|\n",
      "| 22|             67.5|\n",
      "| 20|             67.6|\n",
      "| 19|             70.5|\n",
      "| 17|             61.5|\n",
      "| 23|             72.0|\n",
      "| 25|             72.0|\n",
      "| 24|73.66666666666667|\n",
      "| 29|             85.0|\n",
      "| 21|             72.0|\n",
      "| 30|             80.0|\n",
      "| 18|             87.0|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Age\").mean(\"weight\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72ae2d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sum(Age)|\n",
      "+--------+\n",
      "|     927|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##another type to use aggregate\n",
    "df_pyspark.agg({\"Age\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afbe532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"practise\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d91abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-JNH52G9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b89cf0b950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "768e6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "training=spark.read.csv(\"test2.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c1761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.printSchema()\n",
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982e4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "##making (age,experience) as 1 column--independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7545227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "374a09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureassembler=VectorAssembler(inputCols=[\"age\",\"Experience\"],outputCol=\"independent features\")\n",
    "output=featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25ab5d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+--------------------+\n",
      "|     Name|age|Experience|Salary|independent features|\n",
      "+---------+---+----------+------+--------------------+\n",
      "|    Krish| 31|        10| 30000|         [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|          [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
      "|     Paul| 24|         3| 20000|          [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
      "+---------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e535662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|independent features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data=output.select(\"independent features\",\"Salary\")\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61034fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "##train test data split\n",
    "train_data,test_data=final_data.randomSplit([0.75,0.25])\n",
    "regressor=LinearRegression(featuresCol=\"independent features\",labelCol=\"Salary\")\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c79be5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([2142.8571, -714.2857])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "589bb5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-29285.71428567377"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2af4028b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.regression.LinearRegressionSummary at 0x2b8a0a19f10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results=regressor.evaluate(test_data)\n",
    "pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6058a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|independent features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|          [23.0,2.0]| 18000|18571.428571427467|\n",
      "|          [29.0,4.0]| 20000| 29999.99999999124|\n",
      "|          [30.0,8.0]| 25000|29285.714285712187|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0987e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39564625.850275315"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b94d8c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6290.04180035994"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c050c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"tips.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47e5df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f0122",
   "metadata": {},
   "source": [
    "###Handling categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b62a16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer=StringIndexer(inputCols=[\"sex\",\"smoker\",\"day\",\"time\"],outputCols=[\"sex_indexed\",\"smoker_indexed\",\"day_indexed\",\"time_indexed\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4245499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_indexed|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|         0.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|         0.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        0.0|           0.0|        0.0|         0.0|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_r=indexer.fit(df).transform(df)\n",
    "df_r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e6dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f21389e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureassembler=VectorAssembler(inputCols=[\"tip\",\"size\",\"sex_indexed\",\"smoker_indexed\",\"day_indexed\",\"time_indexed\"],outputCol=\"Independent feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a9c4ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=featureassembler.transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4f60696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+--------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_indexed| Independent feature|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+--------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|[1.01,2.0,1.0,0.0...|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|[1.66,3.0,0.0,0.0...|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|[3.5,3.0,0.0,0.0,...|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[3.31,2.0,0.0,0.0...|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|[3.61,4.0,1.0,0.0...|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|[4.71,4.0,0.0,0.0...|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[2.0,2.0,0.0,0.0,...|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|[3.12,4.0,0.0,0.0...|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[1.96,2.0,0.0,0.0...|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[3.23,2.0,0.0,0.0...|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[1.71,2.0,0.0,0.0...|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|[5.0,4.0,1.0,0.0,...|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[1.57,2.0,0.0,0.0...|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|        0.0|           0.0|        1.0|         0.0|[3.0,4.0,0.0,0.0,...|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|[3.02,2.0,1.0,0.0...|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|[3.92,2.0,0.0,0.0...|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|         0.0|[1.67,3.0,1.0,0.0...|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|[3.71,3.0,0.0,0.0...|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        1.0|           0.0|        1.0|         0.0|[3.5,3.0,1.0,0.0,...|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        0.0|           0.0|        0.0|         0.0|(6,[0,1],[3.35,3.0])|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ecc21b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "| Independent feature|Total_bill|\n",
      "+--------------------+----------+\n",
      "|[1.01,2.0,1.0,0.0...|     16.99|\n",
      "|[1.66,3.0,0.0,0.0...|     10.34|\n",
      "|[3.5,3.0,0.0,0.0,...|     21.01|\n",
      "|[3.31,2.0,0.0,0.0...|     23.68|\n",
      "|[3.61,4.0,1.0,0.0...|     24.59|\n",
      "|[4.71,4.0,0.0,0.0...|     25.29|\n",
      "|[2.0,2.0,0.0,0.0,...|      8.77|\n",
      "|[3.12,4.0,0.0,0.0...|     26.88|\n",
      "|[1.96,2.0,0.0,0.0...|     15.04|\n",
      "|[3.23,2.0,0.0,0.0...|     14.78|\n",
      "|[1.71,2.0,0.0,0.0...|     10.27|\n",
      "|[5.0,4.0,1.0,0.0,...|     35.26|\n",
      "|[1.57,2.0,0.0,0.0...|     15.42|\n",
      "|[3.0,4.0,0.0,0.0,...|     18.43|\n",
      "|[3.02,2.0,1.0,0.0...|     14.83|\n",
      "|[3.92,2.0,0.0,0.0...|     21.58|\n",
      "|[1.67,3.0,1.0,0.0...|     10.33|\n",
      "|[3.71,3.0,0.0,0.0...|     16.29|\n",
      "|[3.5,3.0,1.0,0.0,...|     16.97|\n",
      "|(6,[0,1],[3.35,3.0])|     20.65|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data=output.select(\"Independent feature\",\"Total_bill\")\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e448ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "###train,test data split\n",
    "Train_data,Test_data=finalized_data.randomSplit([0.75,0.25])\n",
    "regressor=LinearRegression(featuresCol=\"Independent feature\",labelCol=\"Total_bill\")\n",
    "regressor=regressor.fit(Train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb746cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([3.0348, 3.5968, -1.5795, 2.8037, -0.2932, -0.6602])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ebfbf2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4179675324584935"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22632ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results=regressor.evaluate(Test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e2aeafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "| Independent feature|Total_bill|        prediction|\n",
      "+--------------------+----------+------------------+\n",
      "|(6,[0,1],[1.25,2.0])|     10.51|12.404961210707175|\n",
      "|(6,[0,1],[1.47,2.0])|     10.77|13.072614724158989|\n",
      "|(6,[0,1],[1.97,2.0])|     12.02|14.590009072913114|\n",
      "| (6,[0,1],[2.0,3.0])|     16.31|18.277806637020046|\n",
      "| (6,[0,1],[3.0,4.0])|     20.45| 24.90934923770998|\n",
      "| (6,[0,1],[3.6,3.0])|     24.06|23.133468553033246|\n",
      "|(6,[0,1],[4.08,2.0])|     17.92|20.993413224655523|\n",
      "| (6,[0,1],[4.3,2.0])|      21.7|21.661066738107337|\n",
      "|[1.0,1.0,1.0,0.0,...|      7.25| 6.470018974843968|\n",
      "|[1.0,2.0,0.0,1.0,...|      12.6|14.449958562027323|\n",
      "|[1.25,2.0,1.0,0.0...|      8.51| 9.578773560665633|\n",
      "|[1.5,2.0,1.0,0.0,...|     26.41|11.584167226779778|\n",
      "|[1.61,2.0,1.0,1.0...|     10.59|14.721688509202897|\n",
      "|[1.66,3.0,0.0,0.0...|     10.34| 16.95273383614637|\n",
      "|[1.73,2.0,0.0,0.0...|      9.78| 12.61496329377405|\n",
      "|[1.98,2.0,0.0,1.0...|     11.02|17.424051485585405|\n",
      "|[2.0,2.0,0.0,0.0,...|     13.13|14.387808090117492|\n",
      "|[2.0,2.0,0.0,1.0,...|     22.67| 17.48474725953557|\n",
      "|[2.0,2.0,1.0,0.0,...|     10.33| 11.85486508379682|\n",
      "|[2.0,2.0,1.0,0.0,...|     14.15| 11.85486508379682|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c5e3154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.708372132896194"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3097b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
